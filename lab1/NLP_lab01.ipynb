{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPog8q3pk3X/i7/+9Hpc9Dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshini-Manchala/NLP_Lab/blob/main/NLP_lab01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gkulpC7LVW7V"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\"ai4bharat/IndicCorpV2\", name=\"indiccorp_v2\", split=\"tel_Telu\",streaming=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "# -----------------------------\n",
        "#  Load Telugu Dataset\n",
        "# -----------------------------\n",
        "print(\"Loading Telugu dataset from Hugging Face...\")\n",
        "ds = load_dataset(\"ai4bharat/IndicCorpV2\", name=\"indiccorp_v2\", split=\"tel_Telu\",streaming=True)\n",
        "\n",
        "# -----------------------------\n",
        "#  Sentence Tokenization\n",
        "# -----------------------------\n",
        "# Split after sentence-ending punctuation (Telugu & common)\n",
        "SENT_SPLIT_RE = re.compile(r'(?<=[।॥.!?])\\s*')\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    sentences = re.split(SENT_SPLIT_RE, text.strip())\n",
        "    return [s for s in sentences if s.strip()]\n",
        "\n",
        "# -----------------------------\n",
        "#  Word Tokenization\n",
        "# -----------------------------\n",
        "TOKEN_RE = re.compile(r\"\"\"\n",
        "    (https?://\\S+)                           # URLs\n",
        "  | ([\\w\\.-]+@[\\w\\.-]+\\.\\w+)                 # Emails\n",
        "  | (\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})          # Dates\n",
        "  | (\\d+(?:\\.\\d+)?)                          # Numbers & decimals\n",
        "  | ([\\u0C00-\\u0C7F]+)                       # Telugu words\n",
        "  | ([A-Za-z]+)                              # English words\n",
        "  | (\\S)                                     # Punctuation / symbols\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "def word_tokenize(sentence):\n",
        "    return [m.group(0) for m in TOKEN_RE.finditer(sentence)]\n",
        "\n",
        "# -----------------------------\n",
        "# 4️⃣ Process, Save, and Compute Statistics\n",
        "# -----------------------------\n",
        "def process_telugu(out_dir=\"telugu_tokenized\", sample_size=None):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    sentences_path = os.path.join(out_dir, \"sentences.txt\")\n",
        "    words_path = os.path.join(out_dir, \"words.txt\")\n",
        "\n",
        "    total_sentences = 0\n",
        "    total_words = 0\n",
        "    total_chars = 0\n",
        "    unique_tokens = set()\n",
        "\n",
        "    # Limit for testing\n",
        "    rows = ds.take(sample_size) if sample_size is not None else ds\n",
        "\n",
        "\n",
        "    with open(sentences_path, \"w\", encoding=\"utf-8\") as fs, \\\n",
        "     open(words_path, \"w\", encoding=\"utf-8\") as fw:\n",
        "        for row in rows:\n",
        "            text = row[\"text\"]\n",
        "\n",
        "        # Sentence tokenization\n",
        "            for sent in sentence_tokenize(text):\n",
        "                fs.write(sent + \"\\n\")\n",
        "                total_sentences += 1\n",
        "\n",
        "            # Word tokenization\n",
        "                tokens = word_tokenize(sent)\n",
        "                for token in tokens:\n",
        "                    fw.write(token + \"\\n\")  # <-- write each token on its own line\n",
        "                total_words += len(tokens)\n",
        "                total_chars += sum(len(t) for t in tokens)\n",
        "                unique_tokens.update(tokens)\n",
        "\n",
        "\n",
        "    # Stats\n",
        "    avg_sent_len = total_words / total_sentences if total_sentences else 0\n",
        "    avg_word_len = total_chars / total_words if total_words else 0\n",
        "    ttr = len(unique_tokens) / total_words if total_words else 0\n",
        "\n",
        "    print(\"\\n--- Corpus Statistics (Telugu) ---\")\n",
        "    print(f\"Total sentences: {total_sentences}\")\n",
        "    print(f\"Total words/tokens: {total_words}\")\n",
        "    print(f\"Total characters in tokens: {total_chars}\")\n",
        "    print(f\"Average sentence length (words): {avg_sent_len:.2f}\")\n",
        "    print(f\"Average word length (characters): {avg_word_len:.2f}\")\n",
        "    print(f\"Type-Token Ratio: {ttr:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5️⃣ Run (Test mode first)\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # For quick test, set sample_size=2000\n",
        "    # For full dataset, set sample_size=None (⚠ huge file!)\n",
        "    process_telugu(out_dir=\"telugu_tokenized\", sample_size=100000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkBESFXyY6id",
        "outputId": "fc78e49e-ded4-4867-a38e-4a7af70d28d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Telugu dataset from Hugging Face...\n",
            "\n",
            "--- Corpus Statistics (Telugu) ---\n",
            "Total sentences: 227481\n",
            "Total words/tokens: 2256541\n",
            "Total characters in tokens: 12112204\n",
            "Average sentence length (words): 9.92\n",
            "Average word length (characters): 5.37\n",
            "Type-Token Ratio: 0.0903\n"
          ]
        }
      ]
    }
  ]
}